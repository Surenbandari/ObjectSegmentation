import os
import random
import matplotlib.pyplot as plt
import cv2
import numpy as np
import torch
import torch.utils.data
import torchvision
from PIL import Image
from pycocotools.coco import COCO

"""# Download and Unzip ARMBench Dataset"""

# !wget https://armbench-dataset.s3.amazonaws.com/segmentation/armbench-segmentation-0.1.tar.gz
# !tar -xzf "/content/armbench-segmentation-0.1.tar.gz"

print('Mix-tote-object Image Data' , len(os.listdir('/content/armbench-segmentation-0.1/mix-object-tote/images')))
print('Same-object-transfer Image Data' , len(os.listdir('/content/armbench-segmentation-0.1/same-object-transfer-set/images')))
print('Zoomed-out-tote Image Data' , len(os.listdir('/content/armbench-segmentation-0.1/zoomed-out-tote-transfer-set/images')))

"""# As per the research paper page 4, Authors clearly mentioned that We observe that applying model weights trained on `mixobject-tote` to the `zoomed-out-tote-transfer-set (mAP50 = 0.25)` and `same-object-transfer-set subsets (mAP50 = 0.11)`

# Custom Dataset with COCO format
"""

class ARMBench(torch.utils.data.Dataset):
  def __init__(self, root, annotation, transforms=None):
    self.root = root
    self.transforms = transforms
    self.coco = COCO(annotation)
    self.ids = list(sorted(self.coco.imgs.keys()))

  def __getitem__(self, index):
    coco = self.coco
    img_id = self.ids[index]
    ann_ids = coco.getAnnIds(imgIds=img_id)
    coco_annotation = coco.loadAnns(ann_ids)
    cat_ids = coco.getCatIds()
    path = coco.loadImgs(img_id)[0]['file_name']
    img = Image.open(os.path.join(self.root, path))
    mask = np.stack([np.where(coco.annToMask(ann)>0,1,0) for ann in coco_annotation])
    num_objs = len(coco_annotation)

    boxes = []
    labels = []
    areas = []
    for i in range(num_objs):
      xmin = coco_annotation[i]['bbox'][0]
      ymin = coco_annotation[i]['bbox'][1]
      xmax = xmin + coco_annotation[i]['bbox'][2]
      ymax = ymin + coco_annotation[i]['bbox'][3]
      boxes.append([xmin, ymin, xmax, ymax])
      labels.append(coco_annotation[i]['category_id'])
      areas.append(coco_annotation[i]['area'])
      masks = mask.transpose((1,2,0))
      boxes = torch.as_tensor(boxes, dtype=torch.float32)
      labels = torch.as_tensor(labels, dtype=torch.int64)
      img_id = torch.tensor([img_id])
      areas = torch.as_tensor(areas, dtype=torch.float32)

      iscrowd = torch.zeros((num_objs,), dtype=torch.int64)

      my_annotation = {}
      my_annotation["boxes"] = boxes
      my_annotation["labels"] = labels
      my_annotation["image_id"] = img_id
      my_annotation["area"] = areas
      my_annotation["iscrowd"] = iscrowd

      if self.transforms is not None:
        img = self.transforms(img)
        masks = self.transforms(masks)
        my_annotation["masks"] = masks
        return img, my_annotation

  def __len__(self):
    return len(self.ids)

"""1. In COCO format, the bounding box is given as [xmin, ymin, width, height]; however, Faster R-CNN in PyTorch expects the bounding box as [xmin, ymin, xmax, ymax].
2. The inputs for a PyTorch model must be in tensor format. I defined get_transform() as below.
"""

def get_transform():
  custom_transforms = []
  custom_transforms.append(torchvision.transforms.ToTensor())
  return torchvision.transforms.Compose(custom_transforms)

"""Create a train and test dataloader using our ARMBench dataset class"""

train_data_dir = '/content/armbench-segmentation-0.1/mix-object-tote/images'
train_armbench = '/content/armbench-segmentation-0.1/mix-object-tote/train.json'
test_armbench = '/content/armbench-segmentation-0.1/mix-object-tote/test.json'

train_dataset = ARMBench(root=train_data_dir, annotation=train_armbench, transforms=get_transform())
test_dataset = ARMBench(root=train_data_dir, annotation=test_armbench, transforms=get_transform())
def collate_fn(batch):
  return tuple(zip(*batch))
train_batch_size = 5
test_batch_size = 1
train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)
test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, collate_fn=collate_fn)

len(train_dataset), len(test_dataset)

print(len(train_dataloader),len(test_dataloader))

"""# Visualize dataset with annotations"""

images,labels = next(iter(train_dataloader))
plt.imshow(images[0].numpy().transpose(1,2,0))

def random_color_masks(image):
  colors = [[0, 255, 0],[0, 0, 255],[255, 0, 0],[0, 255, 255],[255, 255, 0],[255, 0, 255],[80, 70, 180], [250, 80, 190],[245, 145, 50],[70, 150, 250],[50, 190, 190]]
  r = np.zeros_like(image).astype(np.uint8)
  g = np.zeros_like(image).astype(np.uint8)
  b = np.zeros_like(image).astype(np.uint8)
  r[image==1], g[image==1], b[image==1] = colors[random.randrange(0, 10)]
  colored_mask = np.stack([r,g,b], axis=2)
  return colored_mask

def preprocess(img,pred,threshold=0.5):
  INSTANCE_CATEGORY_NAMES = ["Tote","Object"]
  masks = (pred[0]['masks'] ==True).squeeze().detach().cpu().numpy()
  pred_class = [INSTANCE_CATEGORY_NAMES[i-1] for i in list(pred[0]['labels'].cpu().numpy())]
  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().cpu().numpy())]
  return masks, pred_boxes, pred_class

def vis_segmentation(images,labels, threshold=0.9, rect_th=3,
                          text_size=3, text_th=3, url=False):
  masks, boxes, pred_cls = preprocess(images,labels,threshold=threshold)
  img = images[0].numpy().transpose(1,2,0)
  img = img*255
  img = img.astype(np.uint8)
  for i in range(len(masks)):
    rgb_mask = random_color_masks(masks[i])
    img = cv2.addWeighted(img, 1, rgb_mask, 0.5, 1)
    pt1 = tuple(int(x) for x in boxes[i][0])
    pt2 = tuple(int(x) for x in boxes[i][1])
    cv2.rectangle(img, pt1, pt2, color=(0, 255, 0), thickness=rect_th)
    cv2.putText(img, pred_cls[i], pt1, cv2.FONT_HERSHEY_SIMPLEX, text_size, (0, 255, 0), thickness=text_th)
  return img, pred_cls, masks[1]

out,cls,instances = vis_segmentation(images,labels)
plt.imshow(out)

